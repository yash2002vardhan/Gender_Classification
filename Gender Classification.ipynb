{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Man': 0, 'Woman': 1}\n",
      "['Man', 'Woman']\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "data_path='Training'\n",
    "categories=os.listdir(data_path)\n",
    "categories.remove('.DS_Store')\n",
    "labels=[i for i in range(len(categories))]\n",
    "\n",
    "label_dict=dict(zip(categories,labels))\n",
    "\n",
    "print(label_dict)\n",
    "print(categories)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=32\n",
    "data=[]\n",
    "target=[]\n",
    "\n",
    "facedata = \"haarcascade_frontalface_default.xml\"\n",
    "cascade = cv2.CascadeClassifier(facedata)\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    folder_path=os.path.join(data_path,category)\n",
    "    img_names=os.listdir(folder_path)\n",
    "\n",
    "        \n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        img=cv2.imread(img_path)\n",
    "        faces = cascade.detectMultiScale(img)\n",
    "        try:\n",
    "            for f in faces:\n",
    "                x, y, w, h = [v for v in f]\n",
    "                sub_face = img[y:y + h, x:x + w]\n",
    "                gray=cv2.cvtColor(sub_face,cv2.COLOR_BGR2GRAY)           \n",
    "                resized=cv2.resize(gray,(img_size,img_size))\n",
    "                data.append(resized)\n",
    "                target.append(label_dict[category])\n",
    "        except Exception as e:\n",
    "            print('Exception:',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "data=np.array(data)/255.0\n",
    "\n",
    "data=np.reshape(data,(data.shape[0],img_size,img_size,1))\n",
    "target=np.array(target)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "new_target=np_utils.to_categorical(target)\n",
    "\n",
    "np.save('./training/data',data)\n",
    "np.save('./training/target',new_target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data=np.load('./training/data.npy')\n",
    "target=np.load('./training/target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 22:50:08.566582: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "noOfFilters=64\n",
    "sizeOfFilter1=(3,3)\n",
    "sizeOfFilter2=(3,3)\n",
    "sizeOfPool=(2,2)\n",
    "noOfNode=64\n",
    "\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "model.add((Conv2D(64, sizeOfFilter1, input_shape=data.shape[1:],activation='relu')))\n",
    "model.add((Conv2D(64, sizeOfFilter1,activation='relu')))\n",
    "model.add(MaxPooling2D(pool_size=sizeOfPool))\n",
    "\n",
    "model.add((Conv2D(128, sizeOfFilter2,activation='relu')))\n",
    "model.add((Conv2D(128, sizeOfFilter2,activation='relu')))\n",
    "model.add(MaxPooling2D(pool_size=sizeOfPool))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(noOfNode, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data,test_data,train_target,test_target=train_test_split(data,target,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.5415 - accuracy: 0.6894"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 22:50:57.107116: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./training/model-001.model/assets\n",
      "367/367 [==============================] - 36s 96ms/step - loss: 0.5419 - accuracy: 0.6893 - val_loss: 0.2338 - val_accuracy: 0.9130\n",
      "Epoch 2/50\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.2431 - accuracy: 0.9095INFO:tensorflow:Assets written to: ./training/model-002.model/assets\n",
      "367/367 [==============================] - 36s 98ms/step - loss: 0.2431 - accuracy: 0.9095 - val_loss: 0.1697 - val_accuracy: 0.9386\n",
      "Epoch 3/50\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1871 - accuracy: 0.9325INFO:tensorflow:Assets written to: ./training/model-003.model/assets\n",
      "367/367 [==============================] - 35s 96ms/step - loss: 0.1871 - accuracy: 0.9325 - val_loss: 0.1590 - val_accuracy: 0.9407\n",
      "Epoch 4/50\n",
      "367/367 [==============================] - 35s 94ms/step - loss: 0.1665 - accuracy: 0.9411 - val_loss: 0.1850 - val_accuracy: 0.9246\n",
      "Epoch 5/50\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1500 - accuracy: 0.9475INFO:tensorflow:Assets written to: ./training/model-005.model/assets\n",
      "367/367 [==============================] - 35s 96ms/step - loss: 0.1501 - accuracy: 0.9475 - val_loss: 0.1486 - val_accuracy: 0.9512\n",
      "Epoch 6/50\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.9496INFO:tensorflow:Assets written to: ./training/model-006.model/assets\n",
      "367/367 [==============================] - 36s 97ms/step - loss: 0.1404 - accuracy: 0.9493 - val_loss: 0.1465 - val_accuracy: 0.9485\n",
      "Epoch 7/50\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9541INFO:tensorflow:Assets written to: ./training/model-007.model/assets\n",
      "367/367 [==============================] - 36s 97ms/step - loss: 0.1302 - accuracy: 0.9542 - val_loss: 0.1374 - val_accuracy: 0.9574\n",
      "Epoch 8/50\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9574INFO:tensorflow:Assets written to: ./training/model-008.model/assets\n",
      "367/367 [==============================] - 35s 96ms/step - loss: 0.1240 - accuracy: 0.9574 - val_loss: 0.1267 - val_accuracy: 0.9557\n",
      "Epoch 9/50\n",
      "367/367 [==============================] - 35s 94ms/step - loss: 0.1151 - accuracy: 0.9594 - val_loss: 0.1311 - val_accuracy: 0.9540\n",
      "Epoch 10/50\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9604INFO:tensorflow:Assets written to: ./training/model-010.model/assets\n",
      "367/367 [==============================] - 36s 97ms/step - loss: 0.1142 - accuracy: 0.9604 - val_loss: 0.1222 - val_accuracy: 0.9618\n",
      "Epoch 11/50\n",
      "367/367 [==============================] - 35s 96ms/step - loss: 0.1015 - accuracy: 0.9637 - val_loss: 0.1310 - val_accuracy: 0.9604\n",
      "Epoch 12/50\n",
      "367/367 [==============================] - 34s 93ms/step - loss: 0.0991 - accuracy: 0.9634 - val_loss: 0.1319 - val_accuracy: 0.9591\n",
      "Epoch 13/50\n",
      "367/367 [==============================] - 34s 93ms/step - loss: 0.0958 - accuracy: 0.9661 - val_loss: 0.1291 - val_accuracy: 0.9587\n",
      "Epoch 14/50\n",
      "367/367 [==============================] - 34s 94ms/step - loss: 0.0889 - accuracy: 0.9682 - val_loss: 0.1355 - val_accuracy: 0.9604\n",
      "Epoch 15/50\n",
      "367/367 [==============================] - 34s 94ms/step - loss: 0.0846 - accuracy: 0.9702 - val_loss: 0.1300 - val_accuracy: 0.9598\n",
      "Epoch 16/50\n",
      "367/367 [==============================] - 34s 94ms/step - loss: 0.0805 - accuracy: 0.9707 - val_loss: 0.1456 - val_accuracy: 0.9611\n",
      "Epoch 17/50\n",
      "367/367 [==============================] - 34s 94ms/step - loss: 0.0816 - accuracy: 0.9710 - val_loss: 0.1260 - val_accuracy: 0.9635\n",
      "Epoch 18/50\n",
      "367/367 [==============================] - 34s 94ms/step - loss: 0.0715 - accuracy: 0.9753 - val_loss: 0.1394 - val_accuracy: 0.9577\n",
      "Epoch 19/50\n",
      "367/367 [==============================] - 34s 94ms/step - loss: 0.0728 - accuracy: 0.9724 - val_loss: 0.1504 - val_accuracy: 0.9632\n",
      "Epoch 20/50\n",
      "367/367 [==============================] - 35s 94ms/step - loss: 0.0664 - accuracy: 0.9766 - val_loss: 0.1540 - val_accuracy: 0.9580\n",
      "Epoch 21/50\n",
      "367/367 [==============================] - 35s 94ms/step - loss: 0.0662 - accuracy: 0.9771 - val_loss: 0.1327 - val_accuracy: 0.9642\n",
      "Epoch 22/50\n",
      "367/367 [==============================] - 35s 94ms/step - loss: 0.0607 - accuracy: 0.9778 - val_loss: 0.1408 - val_accuracy: 0.9608\n",
      "Epoch 23/50\n",
      "367/367 [==============================] - 35s 94ms/step - loss: 0.0608 - accuracy: 0.9783 - val_loss: 0.1724 - val_accuracy: 0.9526\n",
      "Epoch 24/50\n",
      "367/367 [==============================] - 35s 94ms/step - loss: 0.0560 - accuracy: 0.9791 - val_loss: 0.1593 - val_accuracy: 0.9632\n",
      "Epoch 25/50\n",
      "367/367 [==============================] - 35s 94ms/step - loss: 0.0525 - accuracy: 0.9803 - val_loss: 0.1714 - val_accuracy: 0.9577\n",
      "Epoch 26/50\n",
      "367/367 [==============================] - 35s 95ms/step - loss: 0.0479 - accuracy: 0.9817 - val_loss: 0.1952 - val_accuracy: 0.9598\n",
      "Epoch 27/50\n",
      "367/367 [==============================] - 35s 94ms/step - loss: 0.0502 - accuracy: 0.9801 - val_loss: 0.1785 - val_accuracy: 0.9584\n",
      "Epoch 28/50\n",
      "367/367 [==============================] - 38s 103ms/step - loss: 0.0483 - accuracy: 0.9823 - val_loss: 0.2026 - val_accuracy: 0.9574\n",
      "Epoch 29/50\n",
      "367/367 [==============================] - 37s 100ms/step - loss: 0.0452 - accuracy: 0.9857 - val_loss: 0.1856 - val_accuracy: 0.9621\n",
      "Epoch 30/50\n",
      "367/367 [==============================] - 36s 99ms/step - loss: 0.0376 - accuracy: 0.9869 - val_loss: 0.1843 - val_accuracy: 0.9594\n",
      "Epoch 31/50\n",
      "367/367 [==============================] - 44s 121ms/step - loss: 0.0484 - accuracy: 0.9816 - val_loss: 0.1761 - val_accuracy: 0.9635\n",
      "Epoch 32/50\n",
      "367/367 [==============================] - 43s 118ms/step - loss: 0.0395 - accuracy: 0.9845 - val_loss: 0.1899 - val_accuracy: 0.9649\n",
      "Epoch 33/50\n",
      "367/367 [==============================] - 42s 114ms/step - loss: 0.0382 - accuracy: 0.9872 - val_loss: 0.2246 - val_accuracy: 0.9618\n",
      "Epoch 34/50\n",
      "367/367 [==============================] - 42s 116ms/step - loss: 0.0407 - accuracy: 0.9840 - val_loss: 0.2166 - val_accuracy: 0.9621\n",
      "Epoch 35/50\n",
      "367/367 [==============================] - 42s 115ms/step - loss: 0.0389 - accuracy: 0.9851 - val_loss: 0.2203 - val_accuracy: 0.9628\n",
      "Epoch 36/50\n",
      "367/367 [==============================] - 42s 116ms/step - loss: 0.0406 - accuracy: 0.9866 - val_loss: 0.1879 - val_accuracy: 0.9574\n",
      "Epoch 37/50\n",
      "367/367 [==============================] - 43s 116ms/step - loss: 0.0353 - accuracy: 0.9881 - val_loss: 0.2206 - val_accuracy: 0.9601\n",
      "Epoch 38/50\n",
      "367/367 [==============================] - 43s 116ms/step - loss: 0.0288 - accuracy: 0.9891 - val_loss: 0.2240 - val_accuracy: 0.9635\n",
      "Epoch 39/50\n",
      "367/367 [==============================] - 36s 98ms/step - loss: 0.0324 - accuracy: 0.9878 - val_loss: 0.2517 - val_accuracy: 0.9519\n",
      "Epoch 40/50\n",
      "367/367 [==============================] - 37s 101ms/step - loss: 0.0330 - accuracy: 0.9875 - val_loss: 0.1995 - val_accuracy: 0.9536\n",
      "Epoch 41/50\n",
      "367/367 [==============================] - 41s 112ms/step - loss: 0.0272 - accuracy: 0.9903 - val_loss: 0.2552 - val_accuracy: 0.9601\n",
      "Epoch 42/50\n",
      "367/367 [==============================] - 39s 108ms/step - loss: 0.0293 - accuracy: 0.9882 - val_loss: 0.2779 - val_accuracy: 0.9543\n",
      "Epoch 43/50\n",
      "367/367 [==============================] - 44s 120ms/step - loss: 0.0321 - accuracy: 0.9879 - val_loss: 0.2297 - val_accuracy: 0.9540\n",
      "Epoch 44/50\n",
      "367/367 [==============================] - 44s 120ms/step - loss: 0.0313 - accuracy: 0.9901 - val_loss: 0.2218 - val_accuracy: 0.9601\n",
      "Epoch 45/50\n",
      "367/367 [==============================] - 45s 123ms/step - loss: 0.0328 - accuracy: 0.9878 - val_loss: 0.2079 - val_accuracy: 0.9584\n",
      "Epoch 46/50\n",
      "367/367 [==============================] - 43s 117ms/step - loss: 0.0262 - accuracy: 0.9910 - val_loss: 0.2694 - val_accuracy: 0.9601\n",
      "Epoch 47/50\n",
      "367/367 [==============================] - 43s 118ms/step - loss: 0.0219 - accuracy: 0.9916 - val_loss: 0.2567 - val_accuracy: 0.9570\n",
      "Epoch 48/50\n",
      "367/367 [==============================] - 45s 123ms/step - loss: 0.0281 - accuracy: 0.9907 - val_loss: 0.2804 - val_accuracy: 0.9591\n",
      "Epoch 49/50\n",
      "367/367 [==============================] - 41s 113ms/step - loss: 0.0344 - accuracy: 0.9895 - val_loss: 0.2262 - val_accuracy: 0.9618\n",
      "Epoch 50/50\n",
      "367/367 [==============================] - 39s 106ms/step - loss: 0.0282 - accuracy: 0.9902 - val_loss: 0.2255 - val_accuracy: 0.9638\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('./training/model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n",
    "history=model.fit(train_data,train_target,epochs=50,callbacks=[checkpoint],validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
